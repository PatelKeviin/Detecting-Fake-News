{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection\n",
    "\n",
    "#### Text Analysis (Natural Language Processing) and Classification\n",
    "**Problem to solve:** Do you trust all the news you hear from social media? All news are not real, right? So how will you detect the fake news? A type of yellow journalism, fake news encapsulates pieces of news that may be hoaxes and is generally spread through social media and other online media. This is often done to further or impose certain ideas and is often achieved with political agendas. Such news items may contain false and/or exaggerated claims, and may end up being viralized by algorithms, and users may end up in a filter bubble.\n",
    "\n",
    "**Dataset:** The dataset we’ll use for this python project- we’ll call it news.csv. This dataset has a shape of 7796×4. The first column identifies the news, the second and third are the title and text, and the fourth column has labels denoting whether the news is REAL or FAKE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported!\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "print('Libraries imported!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6335, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>daniel greenfield a shillman journalism fellow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>google pinterest digg linkedin reddit stumbleu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>u s secretary of state john f kerry said monda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>kaydee king kaydeeking november 9 2016 the les...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>it s primary day in new york and front runners...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   news_id                                              title  \\\n",
       "0     8476                       You Can Smell Hillary’s Fear   \n",
       "1    10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2     3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3    10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4      875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL   \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n",
       "4  It's primary day in New York and front-runners...  REAL   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  daniel greenfield a shillman journalism fellow...  \n",
       "1  google pinterest digg linkedin reddit stumbleu...  \n",
       "2  u s secretary of state john f kerry said monda...  \n",
       "3  kaydee king kaydeeking november 9 2016 the les...  \n",
       "4  it s primary day in new york and front runners...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data\n",
    "\n",
    "column_names = ['news_id', 'title', 'text', 'label']\n",
    "df = pd.read_csv('news.csv', names=column_names, header=0)\n",
    "\n",
    "# clean text column, removing all non-ascii characters\n",
    "df['cleaned_text'] = df['text'].apply(lambda text: ' '.join(re.sub('\\W', ' ', text).split()).lower())\n",
    "\n",
    "# get shape and head\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train size:  5068\n",
      "x_test size:  1267\n",
      "6237    FAKE\n",
      "3722    FAKE\n",
      "5774    FAKE\n",
      "336     REAL\n",
      "3622    REAL\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# splitting data into training (80%) and testing sets (20%)\n",
    "\n",
    "labels = df.label\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['cleaned_text'], labels, test_size=0.2, random_state=7)\n",
    "\n",
    "print('x_train size: ', x_train.shape[0])\n",
    "print('x_test size: ', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing Natural Language Text data\n",
    "\n",
    "#### Bag-of-Words Model\n",
    "We cannot work with text directly when using machine learning algorithms. Instead, we need to convert the text to numbers.\n",
    "\n",
    "A simple and effective model for thinking about text documents in machine learning is called the Bag-of-Words Model, or BoW. The model is simple in that it throws away all of the order information in the words and focuses on the occurrence of words in a document.\n",
    "\n",
    "This can be done by assigning each word a unique number (tf-idf weight). Then any document we see can be encoded as a fixed-length vector with the length of the vocabulary of known words. The value in each position in the vector could be filled with a count or frequency (or idf value) of each word in the encoded document.\n",
    "\n",
    "The TfidfVectorizer to learn vocabulary and inverse document frequencies across documents (i.e news text) in the training set and then encode those documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let’s initialize a TfidfVectorizer with stop words from the English language and a maximum document frequency of 0.7 \n",
    "# (terms with a higher document frequency will be discarded). Stop words are the most common words in a language that are \n",
    "# to be filtered out before processing the natural language data. And a TfidfVectorizer turns a collection of raw documents \n",
    "# into a matrix of TF-IDF features.\n",
    "\n",
    "tfidfvectorizer = TfidfVectorizer(stop_words='english', max_df=0.7, use_idf=True)\n",
    "# fit the vectorizer on the train set, learning vocabulary and idf from the training set\n",
    "tfidfvectorizer.fit(x_train)\n",
    "# encode the training set and test set\n",
    "tfidfvectorizer_vectors_xtrain = tfidfvectorizer.transform(x_train).toarray()\n",
    "tfidfvectorizer_vectors_xtest = tfidfvectorizer.transform(x_test).toarray()\n",
    "\n",
    "type(tfidfvectorizer_vectors_xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vocabulary of 61651 words is learned from the documents and each word is assigned a unique integer index in the output vector.\n",
    "\n",
    "The inverse document frequencies are calculated for each word in the vocabulary, assigning the lowest score of 1.0 to the most frequently observed word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size (learned):  61651\n",
      "(61651,)\n"
     ]
    }
   ],
   "source": [
    "# vocabulary and idf values learned by the vectorizer\n",
    "\n",
    "vocabulary = tfidfvectorizer.vocabulary_\n",
    "idf_values = tfidfvectorizer.idf_\n",
    "\n",
    "print('vocabulary size (learned): ', len(vocabulary))\n",
    "print(idf_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5068, 61651)\n"
     ]
    }
   ],
   "source": [
    "# as there are 5068 documents in the training set, the encoded training set will have 5068 rows and 61651 columns \n",
    "# corresponding to the learned vocabulary. Every new document i.e. news post will be encoded using these learned\n",
    "# vocabularies and their corresponding tf-idf values will be calculated and used for the machine learning algorithm later.\n",
    "\n",
    "print(tfidfvectorizer_vectors_xtrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive modelling\n",
    "\n",
    "#### Passive Aggressive Classifier\n",
    "Check out for a [youtube video](https://www.youtube.com/watch?v=TJU8NfDdqNQ) by Victor Lavrenko. Citing him here: The Passive Aggressive (PA) algorithm is perfect for classifying massive streams of text data (e.g. Twitter tweets, news text (in our case)). It's easy to implement and very fast, but does not provide global guarantees like the support-vector machine (SVM).\n",
    "\n",
    "We consider the Online setting, that is we receive examples in a sequential manner. On each round we receive an instance x (x is in R^n) and extend a prediction using our current hypothesis wt. We then receive the true target y and suffer an instantaneous loss based on the discrepancy between yt and our prediction. Our goal is to make the cumulative loss that we suffer small. Finally, we update the hypothesis according to the previous hypothesis and the current example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize, fit a PassiveAggressiveClassifier on the training set vectors and its labels\n",
    "\n",
    "pac = PassiveAggressiveClassifier(max_iter=50)\n",
    "pac.fit(tfidfvectorizer_vectors_xtrain, y_train)\n",
    "\n",
    "# predict on the test set and calculate accuracy of the model\n",
    "y_pred = pac.predict(tfidfvectorizer_vectors_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.9%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[595,  43],\n",
       "       [ 47, 582]], dtype=int64)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model evaluation\n",
    "\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {round(score*100, 2)}%')\n",
    "\n",
    "# to dive deeper into evaluating the classifier, we'll use a confusion matrix - which is a table that is often \n",
    "# used to describe the performance of a classification model (or “classifier”) on a set of test data for which the truth\n",
    "# values are known already\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=['FAKE', 'REAL']).ravel()\n",
    "confusion_matrix(y_test, y_pred, labels=['FAKE', 'REAL'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
